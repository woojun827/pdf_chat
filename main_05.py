# main_05_LCEL_UI.py - ì±—ë´‡ê¸°ëŠ¥
# -----------------------------------------------------------
# ChatPDF (Streamlit + LangChain + Chroma + LCEL)
# main_04_LCEL.pyë¥¼ Streamlit UIë¡œ ë³€í™˜í•œ ë²„ì „
# - PDF ì—…ë¡œë“œ ê¸°ëŠ¥
# - LCEL ë°©ì‹ RAG ì²´ì¸ ì‚¬ìš©
# - ì‹¤ì‹œê°„ ì§ˆì˜ì‘ë‹µ
# -----------------------------------------------------------
# ì‹¤í–‰ ë°©ë²•: streamlit run main_05_LCEL_UI.py
# ì‚¬ì „ ì„¤ì¹˜: pip install streamlit langchain langchain-openai langchain-chroma python-dotenv pypdf

import os
import tempfile
import streamlit as st
from dotenv import load_dotenv

# LangChain ê´€ë ¨ import
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# =========================
# 1. í˜ì´ì§€ ì„¤ì •
# =========================
st.set_page_config(
    page_title="ChatPDF - RAG with LCEL",
    layout="centered"
)

st.title("ğŸ“„ ChatPDF - RAG with LCEL")
st.markdown("### PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸í•´ë³´ì„¸ìš”!")
st.write("---")

# =========================
# 2. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ë° í™•ì¸
# =========================
# load_dotenv()
# api_key = os.getenv('OPENAI_API_KEY')

# if not api_key: # ì˜ˆì™¸ì²˜ë¦¬
#     st.error("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
#     st.stop()

# =========================
# 3. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# =========================
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "persist_dir" not in st.session_state:
    st.session_state.persist_dir = "./db/chromadb_streamlit"
if "processed_file" not in st.session_state:
    st.session_state.processed_file = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# =========================
# 4. PDF ë¬¸ì„œ ì²˜ë¦¬ í•¨ìˆ˜
# =========================
@st.cache_data
def process_pdf(uploaded_file):
    """
    ì—…ë¡œë“œëœ PDFë¥¼ ì²˜ë¦¬í•˜ì—¬ ë¬¸ì„œ ì²­í¬ë¡œ ë³€í™˜
    
    Args:
        uploaded_file: Streamlit file uploaderë¡œë¶€í„° ë°›ì€ íŒŒì¼
    
    Returns:
        list: ë¶„í• ëœ ë¬¸ì„œ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    # 4-1. ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_path = tmp_file.name
    
    # 4-2. PDF ë¡œë“œ
    loader = PyPDFLoader(tmp_path)
    pages = loader.load_and_split()
    
    # 4-3. í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í•  (main_04_LCEL.pyì™€ ë™ì¼í•œ ì„¤ì •)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,      # ì²­í¬ í¬ê¸°
        chunk_overlap=50,    # ì²­í¬ ê°„ ì¤‘ì²©
        length_function=len,
        is_separator_regex=False,
    )
    texts = text_splitter.split_documents(pages)
    
    # 4-4. ì„ì‹œ íŒŒì¼ ì‚­ì œ
    os.unlink(tmp_path)
    
    return texts

# =========================
# 5. ë²¡í„°ìŠ¤í† ì–´ ìƒì„± í•¨ìˆ˜
# =========================
def create_vectorstore(documents, persist_dir):
    """
    ë¬¸ì„œë“¤ë¡œë¶€í„° Chroma ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    
    Args:
        documents: ì„ë² ë”©í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        persist_dir: ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ê²½ë¡œ
    
    Returns:
        Chroma: ìƒì„±ëœ ë²¡í„°ìŠ¤í† ì–´ ê°ì²´
    """
    import chromadb
    
    # 5-1. ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    client = chromadb.PersistentClient(path=persist_dir)
    
    # 5-2. ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ì‚­ì œ (ìƒˆë¡œìš´ PDF ì—…ë¡œë“œ ì‹œ ì´ˆê¸°í™”)
    try:
        client.delete_collection("esg")
    except Exception:
        pass
    
    # 5-3. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    embeddings_model = OpenAIEmbeddings()
    
    # 5-4. ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embeddings_model,
        collection_name="esg",
        client=client,
        persist_directory=persist_dir
    )
    
    return vectorstore

# =========================
# 6. ë¬¸ì„œ í¬ë§· í•¨ìˆ˜
# =========================
def format_docs(docs):
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ëŠ” í•¨ìˆ˜
    
    Args:
        docs: retrieverì—ì„œ ë°˜í™˜ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        str: ê° ë¬¸ì„œì˜ ë‚´ìš©ì„ '\n\n'ë¡œ êµ¬ë¶„í•˜ì—¬ í•©ì¹œ ë¬¸ìì—´
    """
    return "\n\n".join(doc.page_content for doc in docs)

# =========================
# 7. LCEL RAG ì²´ì¸ êµ¬ì„± í•¨ìˆ˜
# =========================
def create_rag_chain(vectorstore, model_name="gpt-4o-mini"):
    """
    LCEL ë°©ì‹ì˜ RAG ì²´ì¸ ìƒì„±
    
    Args:
        vectorstore: ê²€ìƒ‰ì— ì‚¬ìš©í•  ë²¡í„°ìŠ¤í† ì–´
        model_name: ì‚¬ìš©í•  LLM ëª¨ë¸ëª…
    
    Returns:
        LCEL RAG ì²´ì¸ ê°ì²´
    """
    # 7-1. LLM ì„¤ì • (main_04_LCEL.pyì™€ ë™ì¼)
    llm = ChatOpenAI(
        model_name=model_name,
        temperature=0
    )
    
    # 7-2. RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
    template = """ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
    
    prompt = ChatPromptTemplate.from_template(template)
    
    # 7-3. LCEL ì²´ì¸ êµ¬ì„± (íŒŒì´í”„ ì—°ì‚°ì | ì‚¬ìš©)
    rag_chain = (
        {
            # context: retrieverë¡œ ë¬¸ì„œ ê²€ìƒ‰ â†’ format_docsë¡œ í¬ë§·íŒ…
            "context": vectorstore.as_retriever(search_kwargs={"k": 3}) | format_docs,
            # question: ì…ë ¥ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬
            "question": RunnablePassthrough()
        }
        | prompt                # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— contextì™€ question ì‚½ì…
        | llm                   # LLMì— í”„ë¡¬í”„íŠ¸ ì „ë‹¬í•˜ì—¬ ë‹µë³€ ìƒì„±
        | StrOutputParser()     # LLM ì¶œë ¥ì„ ë¬¸ìì—´ë¡œ íŒŒì‹±
    )
    
    return rag_chain

# =========================
# 8. PDF ì—…ë¡œë“œ UI
# =========================
st.sidebar.header("ğŸ“ PDF íŒŒì¼ ì—…ë¡œë“œ")
uploaded_file = st.sidebar.file_uploader(
    "PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
    type=["pdf"],
    help="ë¶„ì„í•  PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”"
)

# PDFê°€ ì—…ë¡œë“œë˜ê³ , ì´ì „ì— ì²˜ë¦¬í•œ íŒŒì¼ê³¼ ë‹¤ë¥¸ ê²½ìš°ì—ë§Œ ì¬ì²˜ë¦¬
if uploaded_file is not None:
    if st.session_state.processed_file != uploaded_file.name:
        with st.spinner("ğŸ“š PDFë¥¼ ì½ê³  ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            # 8-1. PDF ì²˜ë¦¬
            documents = process_pdf(uploaded_file)
            
            # 8-2. ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            st.session_state.vectorstore = create_vectorstore(
                documents,
                st.session_state.persist_dir
            )
            
            # 8-3. ì²˜ë¦¬ëœ íŒŒì¼ëª… ì €ì¥
            st.session_state.processed_file = uploaded_file.name
            
            # 8-4. ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
            st.session_state.chat_history = []
            
        st.sidebar.success(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ!\nì´ ì²­í¬ ìˆ˜: {len(documents)}")
        st.sidebar.info(f"ğŸ“„ {uploaded_file.name}")

# =========================
# 9. ì§ˆì˜ì‘ë‹µ UI
# =========================
if uploaded_file is None:
    st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    st.stop()

if st.session_state.vectorstore is None:
    st.warning("ë¬¸ì„œê°€ ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
    st.stop()

# =========================
# 10. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
# =========================
st.header("ğŸ’¬ PDFì—ê²Œ ì§ˆë¬¸í•˜ê¸°")

# 10-1. ì´ì „ ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for i, chat in enumerate(st.session_state.chat_history):
    with st.chat_message("user"):
        st.write(chat["question"])
    with st.chat_message("assistant"):
        st.write(chat["answer"])

# 10-2. ì§ˆë¬¸ ì…ë ¥
question = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")

if question:
    # 10-3. ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
    with st.chat_message("user"):
        st.write(question)
    
    # 10-4. RAG ì²´ì¸ìœ¼ë¡œ ë‹µë³€ ìƒì„±
    with st.chat_message("assistant"):
        with st.spinner("ğŸ¤” ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
            # RAG ì²´ì¸ ìƒì„± ë° ì‹¤í–‰
            rag_chain = create_rag_chain(
                st.session_state.vectorstore,
                model_name="gpt-4o-mini"
            )
            answer = rag_chain.invoke(question)
            
            # ë‹µë³€ í‘œì‹œ
            st.write(answer)
    
    # 10-5. ì±„íŒ… ê¸°ë¡ì— ì¶”ê°€
    st.session_state.chat_history.append({
        "question": question,
        "answer": answer
    })

# =========================
# 11. ì‚¬ì´ë“œë°” ì¶”ê°€ ì •ë³´
# =========================

st.sidebar.write("---")
st.sidebar.header("âš™ï¸ ì„¤ì •")
st.sidebar.info(f"""
- **ëª¨ë¸**: gpt-4o-mini
- **ì²­í¬ í¬ê¸°**: 500
- **ì²­í¬ ì¤‘ì²©**: 50
- **ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜**: 3ê°œ
""")

# =========================
# 12. ê²€ìƒ‰ëœ ë¬¸ì„œ í™•ì¸ (ì„ íƒì )
# =========================
if st.sidebar.checkbox("ğŸ” ê²€ìƒ‰ëœ ì›ë³¸ ë¬¸ì„œ ë³´ê¸°", value=False):
    if question and st.session_state.vectorstore:
        st.write("---")
        st.subheader("ğŸ“‘ ê²€ìƒ‰ëœ ì›ë³¸ ë¬¸ì„œ")
        
        retriever = st.session_state.vectorstore.as_retriever(search_kwargs={"k": 3})
        retrieved_docs = retriever.invoke(question)
        
        for i, doc in enumerate(retrieved_docs, 1):
            with st.expander(f"ğŸ“„ ë¬¸ì„œ {i}"):
                st.markdown(f"**ë‚´ìš© (ì¼ë¶€):**")
                st.text(doc.page_content[:300] + "...")
                st.markdown(f"**ë©”íƒ€ë°ì´í„°:**")
                st.json(doc.metadata)

